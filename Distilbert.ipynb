{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lHht603w84RN"
      },
      "outputs": [],
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6mXcpO0SSVDG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m venv venv\n",
        "!source venv/bin/activate\n",
        "!pip install tensorflow transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1IWLaDDQUmg1"
      },
      "outputs": [],
      "source": [
        "def process_data(str):\n",
        "  #'/content/drive/MyDrive/~MTSU/research/Corpus/GEN-sarc-notsarc.csv'\n",
        "  dataframe = pd.read_csv(str,encoding='ISO-8859-1')\n",
        "  del dataframe['id']\n",
        "  dataframe['label'] = dataframe['class'].map({'notsarc':0,'sarc':1})\n",
        "  dataframe['label'] = dataframe['label'].astype('category').cat.codes\n",
        "  dataframe = dataframe.dropna(how='any',axis=0)\n",
        "  dataframe = shuffle(dataframe)  \n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BkqhHjougEe8"
      },
      "outputs": [],
      "source": [
        "def train_model(model, x_t, x_v, y_t,  y_v, m_t, m_v):\n",
        "  history = model.fit(model.fit(x_t, y=y_t, batch_size=BERT_BATCH_SIZE, epochs=NB_EPOCHS), validation_data=(x_v,y_v))\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.legend(['Training Loss','Validation Loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('CCE Loss')\n",
        "  plt.show()\n",
        "  plt.plot(history.history['categorical_accuracy'])\n",
        "  plt.plot(history.history['val_categorical_accuracy'])\n",
        "  plt.legend(['Training','Validation'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (P)')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LWHdN7ouSXh6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations, optimizers, losses\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import std\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # suppress Tensorflow messages\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.model_selection import train_test_split,KFold , StratifiedKFold\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss7F-aMMf_ZJ"
      },
      "source": [
        "#DISTILBERT TOKENIZATION\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWLE2RAZAOZ7"
      },
      "source": [
        "EXAMPLE of TOKENIZATION PROCESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hDbv1pMW0yhe"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "bert_log_dir='/content/drive/MyDrive/~MTSU/research/data/Distilbert/Distilbert_Model/'\n",
        "bert_model_save_path='/content/drive/MyDrive/~MTSU/research/data/Distilbert_Model/Distilbert_model.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HG3VlCaI0f6U"
      },
      "outputs": [],
      "source": [
        "BERT_EMB_DIM = 200\n",
        "BERT_CNN_FILTERS = 100\n",
        "BERT_DNN_UNITS = 256\n",
        "BERT_OUTPUT_CLASSES = 2\n",
        "BERT_BATCH_SIZE = 32\n",
        "BERT_DROPOUT_RATE = 0.5\n",
        "MAX_LEN = 200\n",
        "NB_EPOCHS = 4\n",
        "k = 2\n",
        "BATCH_SIZE = 32\n",
        "kf = KFold(n_splits=k, shuffle=False)\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "BERT_callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=bert_model_save_path,save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=False,save_freq=NB_EPOCHS),keras.callbacks.TensorBoard(log_dir=bert_log_dir)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XBezhRrqTNix"
      },
      "outputs": [],
      "source": [
        "sarc_df = process_data('/content/drive/MyDrive/~MTSU/research/Corpus/GEN-sarc-notsarc.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "YOijnyM4VIEO",
        "outputId": "8fb5fc01-b0b6-42a3-e7fd-30a4baf3813a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        class  \\\n",
              "748   notsarc   \n",
              "4152     sarc   \n",
              "4116     sarc   \n",
              "5253  notsarc   \n",
              "5983     sarc   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
              "748                                                                                                                                                                                                                                                                                                                                                                                                         Then why does the NRA use that quote at this link? \\r\\n http://www.nraila.org/Issues/FactSheets/Read.aspx?ID=108   \n",
              "4152                                                                                                                                                                                                                                                                                                                                                      You want to explain to me what the age of consent was in 1st century Palestine?\\r\\nAnd rejoicing really sounds like fear and intimidation to me. emoticonXRolleyes   \n",
              "4116                                                                                                                                                                                                                                                                                                                                                                                 The Fact that you believe there is some sort of salvation in Jebus proves you are the dumb one.. You should stop rejecting reality ....   \n",
              "5253                                                                                                                                                                                                                                                                                                                                                                                                                                            And yet there are plenty of websites written by scientists with the details:   \n",
              "5983  Let me help you with that. emoticonXAngel She's a conservative!\\r\\nI'll bet her upcoming debate with Biden will get far more attention than McCain/Obama did! I'll sure be watching! Word has it that the moderator is an Obama lover. \\r\\nI'm just surprised that so many consider it so important how one performs in these quick witted, personality contests. Just because a person is slightly off the mark in wits doesn't mean he/she would not be a good leader - able to make sound political decisions. \\r\\n   \n",
              "\n",
              "      label  \n",
              "748       0  \n",
              "4152      1  \n",
              "4116      1  \n",
              "5253      0  \n",
              "5983      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1e0a324-4782-492a-bdb9-e4d621bdf295\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>notsarc</td>\n",
              "      <td>Then why does the NRA use that quote at this link? \\r\\n http://www.nraila.org/Issues/FactSheets/Read.aspx?ID=108</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4152</th>\n",
              "      <td>sarc</td>\n",
              "      <td>You want to explain to me what the age of consent was in 1st century Palestine?\\r\\nAnd rejoicing really sounds like fear and intimidation to me. emoticonXRolleyes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4116</th>\n",
              "      <td>sarc</td>\n",
              "      <td>The Fact that you believe there is some sort of salvation in Jebus proves you are the dumb one.. You should stop rejecting reality ....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5253</th>\n",
              "      <td>notsarc</td>\n",
              "      <td>And yet there are plenty of websites written by scientists with the details:</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5983</th>\n",
              "      <td>sarc</td>\n",
              "      <td>Let me help you with that. emoticonXAngel She's a conservative!\\r\\nI'll bet her upcoming debate with Biden will get far more attention than McCain/Obama did! I'll sure be watching! Word has it that the moderator is an Obama lover. \\r\\nI'm just surprised that so many consider it so important how one performs in these quick witted, personality contests. Just because a person is slightly off the mark in wits doesn't mean he/she would not be a good leader - able to make sound political decisions. \\r\\n</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1e0a324-4782-492a-bdb9-e4d621bdf295')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1e0a324-4782-492a-bdb9-e4d621bdf295 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1e0a324-4782-492a-bdb9-e4d621bdf295');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sarc_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6wv21EQlVGFS"
      },
      "outputs": [],
      "source": [
        "X = sarc_df['text'].to_list()\n",
        "Y = sarc_df['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EDtXooPr6LFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split Train and Validation data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Keep some data for inference (testing)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4jpx7C6q6RTh"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfmrz-eBTO2n",
        "outputId": "a97192ac-b608-4eea-cdb7-0538f2abc09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: 'Then why does the NRA use that quote at this link? \r\n",
            " http://www.nraila.org/Issues/FactSheets/Read.aspx?ID=108'\n",
            "input ids: [101, 2059, 2339, 2515, 1996, 17212, 2050, 2224, 2008, 14686, 2012, 2023, 4957, 1029, 8299, 1024, 1013, 1013, 7479, 1012, 17212, 12502, 2050, 1012, 8917, 1013, 3314, 1013, 8866, 21030, 3215, 1013, 3191, 1012, 2004, 2361, 2595, 1029, 8909, 1027, 10715, 102]\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "phrase = X[0]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "inputs = tokenizer(phrase, max_length=MAX_LEN, truncation=True, padding=True)\n",
        "\n",
        "print(f'Phrase: \\'{phrase}\\'')\n",
        "print(f'input ids: {inputs[\"input_ids\"]}')\n",
        "print(f'attention mask: {inputs[\"attention_mask\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3to0I8HJC5xc",
        "outputId": "b2e4dbb5-b560-477d-84e6-bff873558c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# We classify two labels in this example. In case of multiclass classification, adjust num_labels value\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                              num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b4pnsgNmTTdS"
      },
      "outputs": [],
      "source": [
        "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
        "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QzAVFZUEwNuV"
      },
      "outputs": [],
      "source": [
        "encodings = construct_encodings(X, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NAaKvMuYweSg"
      },
      "outputs": [],
      "source": [
        "input_ids, attention_masks = encodings['input_ids'], encodings['attention_mask']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START OF k-FOLD "
      ],
      "metadata": {
        "id": "iP48jAYlruzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZMTivate6X3E"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts,\n",
        "                            truncation=True,\n",
        "                            padding=True, \n",
        "                            return_tensors=\"tf\")\n",
        "val_encodings = tokenizer(val_texts,\n",
        "                          truncation=True,\n",
        "                          padding=True,\n",
        "                          return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Kzr9SC1HCxnr"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l33kz29oC8uo",
        "outputId": "dcd4c82b-0db7-42d6-f5a5-a8b8700c232e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold 1/2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "162/162 [==============================] - 133s 757ms/step - loss: 0.5648 - accuracy: 0.7476 - val_loss: 0.4841 - val_accuracy: 0.7791\n",
            "Epoch 2/3\n",
            "162/162 [==============================] - 121s 748ms/step - loss: 0.3612 - accuracy: 0.8474 - val_loss: 0.5083 - val_accuracy: 0.7745\n",
            "Epoch 3/3\n",
            "162/162 [==============================] - 121s 747ms/step - loss: 0.1862 - accuracy: 0.9330 - val_loss: 0.7295 - val_accuracy: 0.7799\n",
            "[[ 4.757387   0.4234966 -2.9918168 -3.1696095 -2.7723002]]\n",
            "[9.8576051e-01 1.2929648e-02 4.2494724e-04 3.5573001e-04 5.2926078e-04]\n",
            "Training on fold 1/2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_59']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "162/162 [==============================] - 131s 758ms/step - loss: 0.5749 - accuracy: 0.7347 - val_loss: 0.4668 - val_accuracy: 0.7876\n",
            "Epoch 2/3\n",
            "162/162 [==============================] - 121s 747ms/step - loss: 0.3734 - accuracy: 0.8385 - val_loss: 0.4570 - val_accuracy: 0.7983\n",
            "Epoch 3/3\n",
            "162/162 [==============================] - 121s 748ms/step - loss: 0.1793 - accuracy: 0.9345 - val_loss: 0.6832 - val_accuracy: 0.7638\n",
            "[[ 4.9733663  0.839418  -3.564312  -3.6232696 -3.734241 ]]\n",
            "[9.8370451e-01 1.5758457e-02 1.9275174e-04 1.8171598e-04 1.6262932e-04]\n"
          ]
        }
      ],
      "source": [
        "for train_index, val_index in kf.split(train_encodings, val_encodings):\n",
        "      index = 0\n",
        "      print(\"Training on fold \" + str(index+1) + \"/2...\" )   # Generate batches from indices\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "          dict(train_encodings),\n",
        "          train_labels\n",
        "      ))\n",
        "\n",
        "      val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "          dict(val_encodings),\n",
        "          val_labels\n",
        "      ))\n",
        "     \n",
        "      model = None\n",
        "      model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
        "\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "      model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "      model.compile(optimizer=optimizer, \n",
        "                    loss=SparseCategoricalCrossentropy(from_logits=True), \n",
        "                    metrics=['accuracy'])\n",
        "      model.fit(train_dataset.shuffle(100).batch(32),\n",
        "                epochs=3,\n",
        "                batch_size=32,\n",
        "                validation_data=val_dataset.shuffle(100).batch(32))\n",
        "      test_sentence = \"With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15\"\n",
        "      test_sentence_sarcasm = \"News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast\"\n",
        "\n",
        "      # replace to test_sentence_sarcasm variable, if you want to test sarcasm\n",
        "      predict_input = tokenizer.encode(test_sentence,\n",
        "                                      truncation=True,\n",
        "                                      padding=True,\n",
        "                                      return_tensors=\"tf\")\n",
        "\n",
        "      tf_output = model.predict(predict_input)[0]\n",
        "      print(tf_output)\n",
        "\n",
        "      tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
        "      print(tf_prediction)\n",
        "\n",
        "      # 9.9978644e-01 = 0.99978644\n",
        "      # 2.1356659e-04 = 0.00021356659\n",
        "      # => sentiment = 0\n",
        "      predict_input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dmfeNn8hndZs"
      },
      "source": [
        "model.save_pretrained(\"/tmp/sentiment_custom_model\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_jwvD6AUndZu"
      },
      "source": [
        "#### Load saved model and run predict function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s71ZiN0bndZw",
        "outputId": "bdf4f32a-a74b-4959-ad57-eebc112ff66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"/tmp/sentiment_custom_model\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /tmp/sentiment_custom_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /tmp/sentiment_custom_model and are newly initialized: ['dropout_59']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "akGTf-l_ndZy",
        "outputId": "db3057ca-2148-44c0-9878-414075ecd173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_sentence = \"With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15\"\n",
        "test_sentence_sarcasm = \"News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast\"\n",
        "\n",
        "# replace to test_sentence_sarcasm variable, if you want to test sarcasm\n",
        "predict_input = tokenizer.encode(test_sentence,\n",
        "                                 truncation=True,\n",
        "                                 padding=True,\n",
        "                                 return_tensors=\"tf\")\n",
        "\n",
        "tf_output = loaded_model.predict(predict_input)[0]\n",
        "print(tf_output)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.2609034  2.767987  -3.9525511 -4.2107472 -3.9016755]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qlWFuj7HndZ0",
        "outputId": "baa480fe-dcec-4318-b999-d8cdf86a6a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
        "print(tf_prediction)\n",
        "\n",
        "# 9.9978644e-01 = 0.99978644\n",
        "# 2.1356659e-04 = 0.00021356659\n",
        "# => sentiment = 0"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.1600565e-01 1.8336980e-01 2.2112370e-04 1.7080549e-04 2.3266459e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY9UNsxIC7PG",
        "outputId": "6f8a5933-0f5e-4fab-b2ec-86b3578229ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predict_input"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 21), dtype=int32, numpy=\n",
              "array([[  101,  2007,  2037,  5014,  1999, 11289,  1010,  3901,  3745,\n",
              "        24560,  2075,  7122,  1997,  7691,  2044,  5294,  3748, 26332,\n",
              "         3102,  2321,   102]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d1Wiy6DwZrGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Distilbert.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}